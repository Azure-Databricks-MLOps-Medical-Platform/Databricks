{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MLflow Ïã§Ìóò Ìä∏ÎûòÌÇπ ‚Äî P2T2 ÌååÏù¥ÌîÑÎùºÏù∏ Í≤∞Í≥º Î°úÍπÖ\n",
                "\n",
                "> **Purpose:** Gold ÏûÑÏÉÅ ÏöîÏïΩ, SOAP ÎÖ∏Ìä∏, BioMedCLIP Îß§Ïπ≠, Judge ÌèâÍ∞Ä Í≤∞Í≥ºÎ•º\n",
                "> MLflow ExperimentÏóê Í∏∞Î°ùÌïòÏó¨ Î™®Îç∏ ÏÑ±Îä•ÏùÑ Ï∂îÏ†ÅÌï©ÎãàÎã§.\n",
                ">\n",
                "> **Ïπ¥ÌÉàÎ°úÍ∑∏:** `P2T2`  \n",
                "> **ÏÜåÏä§ ÌÖåÏù¥Î∏î:** `gold.patient_clinical_summary`, `ai_results.*`  \n",
                "> **MLflow Ïã§Ìóò:** `/Shared/P2T2_*`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Ïπ¥ÌÉàÎ°úÍ∑∏ Î∞è MLflow ÏÑ§Ï†ï"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import mlflow\n",
                "from mlflow.tracking import MlflowClient\n",
                "from pyspark.sql import functions as F\n",
                "from datetime import datetime\n",
                "import json\n",
                "import tempfile\n",
                "import os\n",
                "\n",
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "\n",
                "client = MlflowClient()\n",
                "\n",
                "# ‚îÄ‚îÄ Ïã§Ìóò Ï†ïÏùò (ÌîåÎû´ Í≤ΩÎ°ú ‚Äî Ï§ëÏ≤© Ìè¥Îçî Î∂àÌïÑÏöî) ‚îÄ‚îÄ\n",
                "EXPERIMENTS = {\n",
                "    \"pipeline_metrics\": \"/Shared/P2T2_Pipeline_Metrics\",\n",
                "    \"clinical_inference\": \"/Shared/P2T2_Clinical_Inference\",\n",
                "    \"judge_evaluation\": \"/Shared/P2T2_Judge_Evaluation\",\n",
                "}\n",
                "\n",
                "for key, name in EXPERIMENTS.items():\n",
                "    exp = mlflow.set_experiment(name)\n",
                "    print(f\"‚úÖ Ïã§Ìóò Ï§ÄÎπÑ: {name} (ID: {exp.experiment_id})\")\n",
                "\n",
                "print(f\"\\nüîß MLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Run 1 ‚Äî Gold Pipeline Î©îÌä∏Î¶≠\n",
                "\n",
                "ÌôòÏûêÎ≥Ñ ÏûÑÏÉÅ ÏöîÏïΩ ÌÜµÍ≥ÑÎ•º MLflowÏóê Í∏∞Î°ùÌï©ÎãàÎã§."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Gold ÌÖåÏù¥Î∏î ÏùΩÍ∏∞\n",
                "df_gold = spark.table(\"P2T2.gold.patient_clinical_summary\")\n",
                "gold_cols = df_gold.columns\n",
                "gold_rows = df_gold.collect()\n",
                "\n",
                "mlflow.set_experiment(EXPERIMENTS[\"pipeline_metrics\"])\n",
                "\n",
                "for row in gold_rows:\n",
                "    pid = row[\"patient_id\"]\n",
                "    \n",
                "    with mlflow.start_run(run_name=f\"gold_summary_{pid}\"):\n",
                "        mlflow.log_params({\n",
                "            \"patient_id\": pid,\n",
                "            \"pipeline_stage\": \"gold\",\n",
                "            \"source_table\": \"P2T2.gold.patient_clinical_summary\",\n",
                "        })\n",
                "        \n",
                "        metrics = {}\n",
                "        metric_cols = [\"avg_heart_rate\", \"avg_systolic_bp\", \"avg_diastolic_bp\",\n",
                "                       \"avg_spo2\", \"avg_temperature\", \"avg_respiratory_rate\",\n",
                "                       \"max_risk_score\", \"avg_risk_score\"]\n",
                "        for col in metric_cols:\n",
                "            if col in gold_cols:\n",
                "                val = row[col]\n",
                "                if val is not None:\n",
                "                    metrics[col] = float(val)\n",
                "        \n",
                "        for cnt_col in [\"vital_count\", \"history_count\"]:\n",
                "            if cnt_col in gold_cols and row[cnt_col] is not None:\n",
                "                metrics[cnt_col] = int(row[cnt_col])\n",
                "        \n",
                "        if metrics:\n",
                "            mlflow.log_metrics(metrics)\n",
                "        \n",
                "        mlflow.set_tags({\"project\": \"P2T2\", \"phase\": \"gold_aggregation\", \"patient_id\": pid})\n",
                "        \n",
                "        run_id = mlflow.active_run().info.run_id\n",
                "        print(f\"‚úÖ Gold [{pid}] logged ‚Äî run_id: {run_id}\")\n",
                "        for k, v in metrics.items():\n",
                "            print(f\"   {k} = {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run 2 ‚Äî BioMedCLIP ÏòÅÏÉÅ Î∂ÑÏÑù\n",
                "\n",
                "BioMedCLIP ÏòÅÏÉÅ-ÌÖçÏä§Ìä∏ Îß§Ïπ≠ Í≤∞Í≥ºÎ•º Í∏∞Î°ùÌï©ÎãàÎã§."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "df_clip = spark.table(\"P2T2.ai_results.biomedclip_results\")\n",
                "clip_cols = df_clip.columns\n",
                "clip_rows = df_clip.collect()\n",
                "\n",
                "mlflow.set_experiment(EXPERIMENTS[\"clinical_inference\"])\n",
                "\n",
                "for row in clip_rows:\n",
                "    pid = row[\"patient_id\"]\n",
                "    \n",
                "    with mlflow.start_run(run_name=f\"biomedclip_{pid}\"):\n",
                "        mlflow.log_params({\n",
                "            \"patient_id\": pid,\n",
                "            \"model_name\": \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n",
                "            \"model_type\": \"multimodal_clip\",\n",
                "            \"source_table\": \"P2T2.ai_results.biomedclip_results\",\n",
                "        })\n",
                "        \n",
                "        if \"top_similarity\" in clip_cols:\n",
                "            sim_val = row[\"top_similarity\"]\n",
                "            if sim_val is not None:\n",
                "                try:\n",
                "                    mlflow.log_metric(\"top_similarity\", float(sim_val))\n",
                "                except (ValueError, TypeError):\n",
                "                    mlflow.log_param(\"top_similarity_str\", str(sim_val)[:250])\n",
                "        \n",
                "        tags = {\"project\": \"P2T2\", \"phase\": \"ai_inference\"}\n",
                "        if \"top_diagnosis\" in clip_cols:\n",
                "            tags[\"top_diagnosis\"] = str(row[\"top_diagnosis\"])[:250]\n",
                "        if \"urgency_level\" in clip_cols:\n",
                "            tags[\"urgency_level\"] = str(row[\"urgency_level\"])\n",
                "        mlflow.set_tags(tags)\n",
                "        \n",
                "        run_id = mlflow.active_run().info.run_id\n",
                "        print(f\"‚úÖ BioMedCLIP [{pid}] logged ‚Äî run_id: {run_id}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Run 3 ‚Äî OpenAI SOAP ÎÖ∏Ìä∏ ÏÉùÏÑ±\n",
                "\n",
                "GPT-5.1 Í∏∞Î∞ò SOAP ÎÖ∏Ìä∏ ÏÉùÏÑ± Í≤∞Í≥ºÎ•º Í∏∞Î°ùÌï©ÎãàÎã§."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "df_soap = spark.table(\"P2T2.ai_results.openai_soap_notes\")\n",
                "soap_cols = df_soap.columns\n",
                "soap_rows = df_soap.collect()\n",
                "\n",
                "mlflow.set_experiment(EXPERIMENTS[\"clinical_inference\"])\n",
                "\n",
                "for row in soap_rows:\n",
                "    pid = row[\"patient_id\"]\n",
                "    \n",
                "    with mlflow.start_run(run_name=f\"soap_note_{pid}\"):\n",
                "        params = {\n",
                "            \"patient_id\": pid,\n",
                "            \"model_type\": \"llm_soap_generation\",\n",
                "            \"source_table\": \"P2T2.ai_results.openai_soap_notes\",\n",
                "        }\n",
                "        if \"model_version\" in soap_cols:\n",
                "            params[\"model_version\"] = str(row[\"model_version\"])\n",
                "        mlflow.log_params(params)\n",
                "        \n",
                "        if \"tokens_used\" in soap_cols:\n",
                "            tokens = row[\"tokens_used\"]\n",
                "            if tokens is not None:\n",
                "                try:\n",
                "                    mlflow.log_metric(\"tokens_used\", float(tokens))\n",
                "                except (ValueError, TypeError):\n",
                "                    mlflow.log_param(\"tokens_used_str\", str(tokens))\n",
                "        \n",
                "        soap_text = \"\"\n",
                "        if \"soap_note\" in soap_cols:\n",
                "            soap_text = str(row[\"soap_note\"] or \"\")\n",
                "        mlflow.log_metric(\"soap_length_chars\", len(soap_text))\n",
                "        mlflow.log_metric(\"soap_length_words\", len(soap_text.split()))\n",
                "        \n",
                "        if soap_text:\n",
                "            with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False, encoding=\"utf-8\") as f:\n",
                "                f.write(f\"Patient: {pid}\\n\")\n",
                "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
                "                f.write(soap_text)\n",
                "                tmp_path = f.name\n",
                "            mlflow.log_artifact(tmp_path, artifact_path=\"soap_notes\")\n",
                "            os.unlink(tmp_path)\n",
                "        \n",
                "        mlflow.set_tags({\"project\": \"P2T2\", \"phase\": \"soap_generation\", \"patient_id\": pid})\n",
                "        \n",
                "        run_id = mlflow.active_run().info.run_id\n",
                "        print(f\"‚úÖ SOAP [{pid}] logged ‚Äî run_id: {run_id}, {len(soap_text)} chars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run 4 ‚Äî LLM-as-a-Judge ÌèâÍ∞Ä\n",
                "\n",
                "Judge ÌèâÍ∞Ä Ï†êÏàòÎ•º Í∏∞Î°ùÌï©ÎãàÎã§."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "df_judge = spark.table(\"P2T2.ai_results.judge_evaluation\")\n",
                "judge_cols = df_judge.columns\n",
                "judge_rows = df_judge.collect()\n",
                "\n",
                "mlflow.set_experiment(EXPERIMENTS[\"judge_evaluation\"])\n",
                "\n",
                "for row in judge_rows:\n",
                "    pid = row[\"patient_id\"]\n",
                "    \n",
                "    with mlflow.start_run(run_name=f\"judge_eval_{pid}\"):\n",
                "        params = {\n",
                "            \"patient_id\": pid,\n",
                "            \"model_type\": \"llm_as_a_judge\",\n",
                "            \"source_table\": \"P2T2.ai_results.judge_evaluation\",\n",
                "        }\n",
                "        if \"judge_model\" in judge_cols:\n",
                "            params[\"judge_model\"] = str(row[\"judge_model\"])\n",
                "        mlflow.log_params(params)\n",
                "        \n",
                "        if \"overall_score\" in judge_cols:\n",
                "            overall = row[\"overall_score\"]\n",
                "            if overall is not None:\n",
                "                try:\n",
                "                    mlflow.log_metric(\"overall_score\", float(overall))\n",
                "                except (ValueError, TypeError):\n",
                "                    mlflow.log_param(\"overall_score_str\", str(overall))\n",
                "        \n",
                "        if \"confidence\" in judge_cols:\n",
                "            conf = row[\"confidence\"]\n",
                "            if conf is not None:\n",
                "                try:\n",
                "                    mlflow.log_metric(\"confidence\", float(conf))\n",
                "                except (ValueError, TypeError):\n",
                "                    mlflow.log_param(\"confidence_str\", str(conf))\n",
                "        \n",
                "        if \"evaluation_json\" in judge_cols:\n",
                "            eval_json = row[\"evaluation_json\"]\n",
                "            if eval_json:\n",
                "                try:\n",
                "                    eval_data = json.loads(eval_json) if isinstance(eval_json, str) else eval_json\n",
                "                    for k, v in eval_data.items():\n",
                "                        if isinstance(v, (int, float)):\n",
                "                            mlflow.log_metric(f\"judge_{k}\", float(v))\n",
                "                    \n",
                "                    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False, encoding=\"utf-8\") as f:\n",
                "                        json.dump(eval_data, f, ensure_ascii=False, indent=2)\n",
                "                        tmp_path = f.name\n",
                "                    mlflow.log_artifact(tmp_path, artifact_path=\"judge_details\")\n",
                "                    os.unlink(tmp_path)\n",
                "                except (json.JSONDecodeError, TypeError):\n",
                "                    mlflow.log_param(\"evaluation_raw\", str(eval_json)[:250])\n",
                "        \n",
                "        tags = {\"project\": \"P2T2\", \"phase\": \"judge_evaluation\", \"patient_id\": pid}\n",
                "        if \"pass_fail\" in judge_cols:\n",
                "            tags[\"pass_fail\"] = str(row[\"pass_fail\"])\n",
                "        mlflow.set_tags(tags)\n",
                "        \n",
                "        run_id = mlflow.active_run().info.run_id\n",
                "        overall_val = row[\"overall_score\"] if \"overall_score\" in judge_cols else \"N/A\"\n",
                "        pass_fail = row[\"pass_fail\"] if \"pass_fail\" in judge_cols else \"N/A\"\n",
                "        print(f\"‚úÖ Judge [{pid}] logged ‚Äî run_id: {run_id}\")\n",
                "        print(f\"   Score: {overall_val}/5, Pass/Fail: {pass_fail}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Ïã§Ìóò ÏöîÏïΩ Î¶¨Ìè¨Ìä∏"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"üìä P2T2 MLflow Ïã§Ìóò Îì±Î°ù ÏöîÏïΩ\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "total_runs = 0\n",
                "for key, name in EXPERIMENTS.items():\n",
                "    exp = client.get_experiment_by_name(name)\n",
                "    if exp:\n",
                "        runs = client.search_runs(experiment_ids=[exp.experiment_id])\n",
                "        total_runs += len(runs)\n",
                "        print(f\"\\n  üìÅ {name}\")\n",
                "        print(f\"     Runs: {len(runs)}\")\n",
                "        for r in runs:\n",
                "            run_name = r.data.tags.get(\"mlflow.runName\", \"unnamed\")\n",
                "            metrics_str = \", \".join(\n",
                "                f\"{k}={v:.3f}\" for k, v in sorted(r.data.metrics.items())[:5]\n",
                "            )\n",
                "            print(f\"     ‚Ä¢ {run_name}: {metrics_str}\")\n",
                "    else:\n",
                "        print(f\"\\n  ‚ö™ {name}: Ïã§Ìóò ÏóÜÏùå\")\n",
                "\n",
                "print(f\"\\n{'=' * 70}\")\n",
                "print(f\"  Total: {total_runs} runs across {len(EXPERIMENTS)} experiments\")\n",
                "print(f\"  MLflow UI: {mlflow.get_tracking_uri()}\")\n",
                "print(\"=\" * 70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}