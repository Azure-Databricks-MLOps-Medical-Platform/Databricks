{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gold Layer: 환자 임상 요약 생성\n",
                "\n",
                "> **Purpose:** Silver Layer의 정제된 데이터를 결합/집계하여 AI 추론용 최종 테이블을 생성합니다.\n",
                "> - 환자별 바이탈 통계 집계 (avg, max, count)\n",
                "> - 진료 기록(diagnosis, medication) 결합\n",
                "> - AI 추론 입력 데이터셋 생성\n",
                ">\n",
                "> **카탈로그:** `P2T2.gold.patient_clinical_summary`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. 카탈로그 설정"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "print(\"✅ Catalog: P2T2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 라이브러리"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.window import Window"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 환자별 바이탈 통계 집계"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def aggregate_patient_vitals():\n",
                "    \"\"\"\n",
                "    silver.cleaned_vital_signs를 환자별로 집계합니다.\n",
                "    \n",
                "    실제 컬럼: patient_id, timestamp, heart_rate, systolic_bp,\n",
                "              diastolic_bp, spo2, temperature, respiratory_rate, risk_score\n",
                "    \"\"\"\n",
                "    df_vitals = spark.table(\"P2T2.silver.cleaned_vital_signs\")\n",
                "    print(f\"Silver vital_signs: {df_vitals.count():,} rows\")\n",
                "    \n",
                "    # 환자별 집계\n",
                "    df_agg = df_vitals.groupBy(\"patient_id\").agg(\n",
                "        F.round(F.avg(\"heart_rate\"), 1).alias(\"avg_heart_rate\"),\n",
                "        F.round(F.avg(\"systolic_bp\"), 1).alias(\"avg_systolic_bp\"),\n",
                "        F.round(F.avg(\"diastolic_bp\"), 1).alias(\"avg_diastolic_bp\"),\n",
                "        F.round(F.avg(\"spo2\"), 1).alias(\"avg_spo2\"),\n",
                "        F.round(F.avg(\"temperature\"), 2).alias(\"avg_temperature\"),\n",
                "        F.round(F.avg(\"respiratory_rate\"), 1).alias(\"avg_respiratory_rate\"),\n",
                "        F.round(F.max(\"risk_score\"), 2).alias(\"max_risk_score\"),\n",
                "        F.round(F.avg(\"risk_score\"), 2).alias(\"avg_risk_score\"),\n",
                "        F.count(\"*\").alias(\"vital_count\"),\n",
                "    )\n",
                "    \n",
                "    return df_agg\n",
                "\n",
                "v_agg = aggregate_patient_vitals()\n",
                "v_agg.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 진료 기록 결합"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def join_medical_history(df_vitals_agg):\n",
                "    \"\"\"\n",
                "    바이탈 집계와 silver.cleaned_medical_history를 결합합니다.\n",
                "    \n",
                "    실제 컬럼: patient_id, record_date, hospital, department,\n",
                "              diagnosis, medication, notes\n",
                "    \"\"\"\n",
                "    h = spark.table(\"P2T2.silver.cleaned_medical_history\")\n",
                "    \n",
                "    h_agg = h.groupBy(\"patient_id\").agg(\n",
                "        F.count(\"*\").alias(\"history_count\"),\n",
                "        F.concat_ws(\", \", F.collect_list(\"diagnosis\")).alias(\"diagnoses\"),\n",
                "        F.concat_ws(\", \", F.collect_list(\"medication\")).alias(\"medications\"),\n",
                "    )\n",
                "    \n",
                "    df_joined = df_vitals_agg.join(h_agg, \"patient_id\", \"left\")\n",
                "    return df_joined\n",
                "\n",
                "gold = join_medical_history(v_agg)\n",
                "print(f\"Gold join 결과: {gold.count()} rows\")\n",
                "gold.show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Gold Layer 테이블 생성"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# 생성 시각 추가\n",
                "gold = gold.withColumn(\"aggregated_at\", F.current_timestamp())\n",
                "\n",
                "# 기존 테이블 삭제 후 저장 (스키마 충돌 방지)\n",
                "spark.sql(\"DROP TABLE IF EXISTS P2T2.gold.patient_clinical_summary\")\n",
                "\n",
                "gold.write \\\n",
                "    .format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .saveAsTable(\"P2T2.gold.patient_clinical_summary\")\n",
                "\n",
                "count = spark.table(\"P2T2.gold.patient_clinical_summary\").count()\n",
                "print(f\"✅ Gold Layer 생성 완료: {count:,} patients\")\n",
                "print(f\"   → P2T2.gold.patient_clinical_summary\")\n",
                "\n",
                "# 컬럼 확인\n",
                "print(f\"   Columns: {spark.table('P2T2.gold.patient_clinical_summary').columns}\")\n",
                "spark.table(\"P2T2.gold.patient_clinical_summary\").show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. AI 추론 입력 데이터 추출"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def get_patients_for_ai_inference(limit=100):\n",
                "    \"\"\"\n",
                "    AI 추론(OpenAI API)에 전달할 환자 데이터를 추출합니다.\n",
                "    위험도가 높은 환자 우선 처리\n",
                "    \"\"\"\n",
                "    df = spark.sql(f\"\"\"\n",
                "        SELECT \n",
                "            patient_id,\n",
                "            avg_heart_rate,\n",
                "            avg_systolic_bp,\n",
                "            avg_diastolic_bp,\n",
                "            avg_spo2,\n",
                "            avg_temperature,\n",
                "            avg_respiratory_rate,\n",
                "            max_risk_score,\n",
                "            diagnoses,\n",
                "            medications\n",
                "        FROM P2T2.gold.patient_clinical_summary\n",
                "        ORDER BY max_risk_score DESC\n",
                "        LIMIT {limit}\n",
                "    \"\"\")\n",
                "    \n",
                "    print(f\"✅ AI 추론 대기 환자: {df.count()}명 (위험도순)\")\n",
                "    df.show(truncate=False)\n",
                "    return df\n",
                "\n",
                "df_ai_input = get_patients_for_ai_inference(100)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}