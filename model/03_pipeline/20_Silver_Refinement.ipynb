{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Silver Layer: ë°ì´í„° ì •ì œ ë° Risk Score ê³„ì‚°\n",
                "\n",
                "> **Purpose:** P2T2.bronzeì˜ ì›ì‹œ ë°ì´í„°ë¥¼ ì •ì œ, í•„í„°ë§í•˜ì—¬ P2T2.silverì— ì ì¬í•©ë‹ˆë‹¤.\n",
                "> - Null ê°’ ì œê±° ë° ì´ìƒì¹˜ í•„í„°ë§\n",
                "> - ìœ„í—˜ë„ ìŠ¤ì½”ì–´(risk_score) ê³„ì‚° (0.0 ~ 1.0)\n",
                "> - ì¤‘ë³µ ë ˆì½”ë“œ ì œê±°\n",
                ">\n",
                "> **ì‹¤ì œ ì»¬ëŸ¼ëª…:** `heart_rate`, `systolic_bp`, `diastolic_bp`, `spo2`, `temperature`, `respiratory_rate`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. ì¹´íƒˆë¡œê·¸ ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "print(\"âœ… Catalog: P2T2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import *\n",
                "from pyspark.sql.window import Window"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ë°”ì´íƒˆ ì •ì œ (bronze.vital_signs â†’ silver.cleaned_vital_signs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def refine_vital_signs():\n",
                "    \"\"\"\n",
                "    Bronze ë°”ì´íƒˆ ë¡œê·¸ë¥¼ ì •ì œí•˜ì—¬ Silver Layerì— ì ì¬í•©ë‹ˆë‹¤.\n",
                "    \n",
                "    ì‹¤ì œ ì»¬ëŸ¼: patient_id, timestamp, heart_rate, systolic_bp, \n",
                "              diastolic_bp, spo2, temperature, respiratory_rate\n",
                "    \"\"\"\n",
                "    df_bronze = spark.table(\"P2T2.bronze.vital_signs\")\n",
                "    print(f\"Bronze vital_signs: {df_bronze.count():,} rows\")\n",
                "    print(f\"Columns: {df_bronze.columns}\")\n",
                "    \n",
                "    # 1. Null í–‰ ì œê±°\n",
                "    df_clean = df_bronze.dropna(subset=[\n",
                "        \"heart_rate\", \"systolic_bp\", \"diastolic_bp\", \n",
                "        \"spo2\", \"temperature\", \"respiratory_rate\"\n",
                "    ])\n",
                "    \n",
                "    # 2. ë¬¼ë¦¬ì  ì´ìƒì¹˜ í•„í„°ë§\n",
                "    df_filtered = df_clean.filter(\n",
                "        (F.col(\"heart_rate\").between(20, 250)) &\n",
                "        (F.col(\"systolic_bp\").between(50, 300)) &\n",
                "        (F.col(\"diastolic_bp\").between(20, 200)) &\n",
                "        (F.col(\"spo2\").between(50, 100)) &\n",
                "        (F.col(\"temperature\").between(30.0, 45.0)) &\n",
                "        (F.col(\"respiratory_rate\").between(4, 60))\n",
                "    )\n",
                "    \n",
                "    # 3. ìœ„í—˜ë„ ìŠ¤ì½”ì–´ ê³„ì‚° (0.0 ~ 1.0)\n",
                "    df_scored = df_filtered.withColumn(\n",
                "        \"risk_score\",\n",
                "        (\n",
                "            # ì‹¬ë°•ìˆ˜ ìœ„í—˜ë„ (ì •ìƒ 60-100)\n",
                "            F.when(F.col(\"heart_rate\") > 150, 0.3)\n",
                "             .when(F.col(\"heart_rate\") > 120, 0.2)\n",
                "             .when(F.col(\"heart_rate\") > 100, 0.1)\n",
                "             .when(F.col(\"heart_rate\") < 50, 0.2)\n",
                "             .otherwise(0.0)\n",
                "            +\n",
                "            # í˜ˆì•• ìœ„í—˜ë„ (ì •ìƒ 90-140)\n",
                "            F.when(F.col(\"systolic_bp\") > 180, 0.3)\n",
                "             .when(F.col(\"systolic_bp\") > 160, 0.2)\n",
                "             .when(F.col(\"systolic_bp\") > 140, 0.1)\n",
                "             .when(F.col(\"systolic_bp\") < 80, 0.3)\n",
                "             .otherwise(0.0)\n",
                "            +\n",
                "            # ì‚°ì†Œí¬í™”ë„ ìœ„í—˜ë„ (ì •ìƒ 95-100)\n",
                "            F.when(F.col(\"spo2\") < 85, 0.4)\n",
                "             .when(F.col(\"spo2\") < 90, 0.3)\n",
                "             .when(F.col(\"spo2\") < 94, 0.1)\n",
                "             .otherwise(0.0)\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    # 4. ì¤‘ë³µ ì œê±°\n",
                "    df_dedup = df_scored.dropDuplicates([\"timestamp\", \"heart_rate\", \"systolic_bp\"])\n",
                "    \n",
                "    # 5. ì²˜ë¦¬ ì‹œê° ì¶”ê°€\n",
                "    df_final = df_dedup.withColumn(\"processed_at\", F.current_timestamp())\n",
                "    \n",
                "    # Silver ì €ì¥\n",
                "    df_final.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .saveAsTable(\"P2T2.silver.cleaned_vital_signs\")\n",
                "    \n",
                "    silver_cnt = spark.table(\"P2T2.silver.cleaned_vital_signs\").count()\n",
                "    bronze_cnt = df_bronze.count()\n",
                "    removed = bronze_cnt - silver_cnt\n",
                "    print(f\"âœ… ë°”ì´íƒˆ ì •ì œ ì™„ë£Œ\")\n",
                "    print(f\"   Bronze: {bronze_cnt:,} â†’ Silver: {silver_cnt:,} ({removed:,} removed)\")\n",
                "    df_final.select(\"patient_id\", \"heart_rate\", \"spo2\", \"systolic_bp\", \"risk_score\").show(5)\n",
                "\n",
                "refine_vital_signs()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ê¸°íƒ€ í…Œì´ë¸” ì •ì œ (DICOM, Emergency, Medical History)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# DICOM, Emergency, Medical HistoryëŠ” ê°„ë‹¨ ì •ì œ (ë©”íƒ€ë°ì´í„° ì¶”ê°€)\n",
                "for table in [\"dicom_metadata\", \"emergency_data\", \"medical_history\"]:\n",
                "    df = spark.table(f\"P2T2.bronze.{table}\")\n",
                "    df = df.withColumn(\"processed_at\", F.current_timestamp())\n",
                "    \n",
                "    target = f\"P2T2.silver.cleaned_{table}\"\n",
                "    df.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .saveAsTable(target)\n",
                "    \n",
                "    cnt = spark.table(target).count()\n",
                "    print(f\"âœ… {target}: {cnt:,} rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Silver í’ˆì§ˆ ë¦¬í¬íŠ¸"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ğŸ” P2T2.silver í’ˆì§ˆ ë¦¬í¬íŠ¸\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# ë°”ì´íƒˆ í†µê³„\n",
                "df_vitals = spark.table(\"P2T2.silver.cleaned_vital_signs\")\n",
                "print(f\"\\nğŸ“Š cleaned_vital_signs: {df_vitals.count():,} rows\")\n",
                "\n",
                "# ìœ„í—˜ë„ ë¶„í¬\n",
                "risk_dist = df_vitals.groupBy(\n",
                "    F.when(F.col(\"risk_score\") >= 0.5, \"ğŸ”´ Critical\")\n",
                "     .when(F.col(\"risk_score\") >= 0.2, \"ğŸŸ¡ Warning\")\n",
                "     .otherwise(\"ğŸŸ¢ Normal\").alias(\"risk_level\")\n",
                ").count().collect()\n",
                "\n",
                "for row in risk_dist:\n",
                "    print(f\"   {row['risk_level']}: {row['count']:,}\")\n",
                "\n",
                "# ì „ì²´ Silver í…Œì´ë¸”\n",
                "print(\"\\nğŸ“‹ ì „ì²´ Silver í…Œì´ë¸”:\")\n",
                "for t in spark.sql(\"SHOW TABLES IN silver\").collect():\n",
                "    name = t['tableName']\n",
                "    cnt = spark.table(f\"silver.{name}\").count()\n",
                "    print(f\"   silver.{name}: {cnt:,} rows\")\n",
                "\n",
                "print(\"=\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}