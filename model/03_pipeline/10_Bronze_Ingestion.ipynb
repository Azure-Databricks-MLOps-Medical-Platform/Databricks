{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Bronze Layer: DBFS CSV â†’ P2T2.bronze Delta ì ì¬\n",
                "\n",
                "> **Purpose:** DBFSì— ì—…ë¡œë“œëœ CSV/ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ Bronze Layer Delta í…Œì´ë¸”ë¡œ ì ì¬í•©ë‹ˆë‹¤.\n",
                "> - í™˜ì ë°”ì´íƒˆ CSV (`vital_signs.csv`)\n",
                "> - DICOM ë©”íƒ€ë°ì´í„° CSV (`dicom_metadata.csv`)\n",
                "> - ì‘ê¸‰ ë°ì´í„° CSV (`emergency_data.csv`)\n",
                "> - ì§„ë£Œ ê¸°ë¡ CSV (`medical_history.csv`)\n",
                ">\n",
                "> **ì¹´íƒˆë¡œê·¸:** `P2T2` (Unity Catalog)\n",
                "> **ìŠ¤í‚¤ë§ˆ:** `bronze`\n",
                "> **ì €ì¥ ìœ„ì¹˜:** ADLS `2dt015-personal`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. ì¹´íƒˆë¡œê·¸ ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "print(\"âœ… Catalog: P2T2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import *\n",
                "from datetime import datetime\n",
                "\n",
                "# DBFS ê²½ë¡œ (SDKë¡œ ì—…ë¡œë“œëœ ìœ„ì¹˜)\n",
                "DBFS_BASE = \"dbfs:/patient_data\"\n",
                "\n",
                "# Bronze í…Œì´ë¸” ë§¤í•‘\n",
                "TABLES = {\n",
                "    \"vital_signs\":    f\"{DBFS_BASE}/vital_signs.csv\",\n",
                "    \"dicom_metadata\":  f\"{DBFS_BASE}/dicom_metadata.csv\",\n",
                "    \"emergency_data\":  f\"{DBFS_BASE}/emergency_data.csv\",\n",
                "    \"medical_history\": f\"{DBFS_BASE}/medical_history.csv\",\n",
                "}\n",
                "\n",
                "print(f\"ğŸ”§ Bronze Ingestion ì„¤ì •: {len(TABLES)}ê°œ í…Œì´ë¸”\")\n",
                "for name, path in TABLES.items():\n",
                "    print(f\"   {name} â† {path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. CSV â†’ Bronze Delta ì ì¬"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def ingest_csv_to_bronze(table_name, csv_path):\n",
                "    \"\"\"\n",
                "    DBFSì˜ CSV íŒŒì¼ì„ P2T2.bronze Delta í…Œì´ë¸”ë¡œ ì ì¬í•©ë‹ˆë‹¤.\n",
                "    \n",
                "    Args:\n",
                "        table_name: í…Œì´ë¸”ëª… (ì˜ˆ: vital_signs)\n",
                "        csv_path: DBFS CSV ê²½ë¡œ\n",
                "    \"\"\"\n",
                "    df = (spark.read\n",
                "        .option(\"header\", \"true\")\n",
                "        .option(\"inferSchema\", \"true\")\n",
                "        .csv(csv_path)\n",
                "    )\n",
                "    \n",
                "    # ì ì¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
                "    df = df.withColumn(\"processed_at\", F.current_timestamp())\n",
                "    \n",
                "    # Delta í…Œì´ë¸” ì €ì¥\n",
                "    full_table = f\"P2T2.bronze.{table_name}\"\n",
                "    df.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .saveAsTable(full_table)\n",
                "    \n",
                "    count = spark.table(full_table).count()\n",
                "    print(f\"âœ… {full_table}: {count:,} rows\")\n",
                "    return count\n",
                "\n",
                "# ì‹¤í–‰\n",
                "total = 0\n",
                "for name, path in TABLES.items():\n",
                "    total += ingest_csv_to_bronze(name, path)\n",
                "\n",
                "print(f\"\\nğŸ“¦ Bronze ì ì¬ ì™„ë£Œ: ì´ {total:,} rows, {len(TABLES)}ê°œ í…Œì´ë¸”\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ì ì¬ ìƒíƒœ ìš”ì•½"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"ğŸ“¦ P2T2.bronze ì ì¬ í˜„í™©\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "tables = spark.sql(\"SHOW TABLES IN bronze\").collect()\n",
                "for t in tables:\n",
                "    name = t['tableName']\n",
                "    cnt = spark.table(f\"bronze.{name}\").count()\n",
                "    cols = spark.table(f\"bronze.{name}\").columns\n",
                "    print(f\"  âœ… bronze.{name}: {cnt:,} rows, {len(cols)} columns\")\n",
                "    print(f\"     Columns: {cols}\")\n",
                "\n",
                "print(\"=\" * 60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}