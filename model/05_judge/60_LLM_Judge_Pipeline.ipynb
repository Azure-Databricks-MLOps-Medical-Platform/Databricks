{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM-as-a-Judge ÌíàÏßà ÌèâÍ∞Ä ÌååÏù¥ÌîÑÎùºÏù∏\n",
                "\n",
                "> **Purpose:** AIÍ∞Ä ÏÉùÏÑ±Ìïú SOAP ÎÖ∏Ìä∏Ïùò ÏûÑÏÉÅÏ†Å ÌÉÄÎãπÏÑ±Í≥º Ïã†Î¢∞ÏÑ±ÏùÑ\n",
                "> LLM-as-a-Judge Í∏∞Î≤ïÏúºÎ°ú Í∞ùÍ¥ÄÏ†ÅÏúºÎ°ú Í≤ÄÏ¶ùÌï©ÎãàÎã§.\n",
                ">\n",
                "> **ÌèâÍ∞Ä Í∏∞Ï§Ä (Í∞Å 1-5Ï†ê):**\n",
                "> 1. Clinical Accuracy (Ï†ïÌôïÏÑ±)\n",
                "> 2. Completeness (ÏôÑÏ†ÑÏÑ±)\n",
                "> 3. Actionability (Ïã§ÌñâÍ∞ÄÎä•ÏÑ±)\n",
                "> 4. Safety (ÏïàÏ†ÑÏÑ±)\n",
                "> 5. Relevance (Í¥ÄÎ†®ÏÑ±)\n",
                ">\n",
                "> **Ïπ¥ÌÉàÎ°úÍ∑∏:** `P2T2` ‚Üí `ai_results.judge_evaluation`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Ïπ¥ÌÉàÎ°úÍ∑∏ ÏÑ§Ï†ï"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "print(\"‚úÖ Catalog: P2T2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ Î∞è ÏÑ§Ï†ï"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import openai\n",
                "import json\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import *\n",
                "\n",
                "# Azure Key VaultÏóêÏÑú API ÌÇ§ Î°úÎìú\n",
                "JUDGE_API_KEY = dbutils.secrets.get(scope=\"key-vault-scope\", key=\"openai-api-key\")\n",
                "JUDGE_ENDPOINT = dbutils.secrets.get(scope=\"key-vault-scope\", key=\"openai-endpoint\")\n",
                "\n",
                "client = openai.AzureOpenAI(\n",
                "    azure_endpoint=JUDGE_ENDPOINT,\n",
                "    api_key=JUDGE_API_KEY,\n",
                "    api_version=\"2025-01-01-preview\"\n",
                ")\n",
                "JUDGE_MODEL = \"gpt-51-deploy\"\n",
                "\n",
                "print(\"üîß LLM-as-a-Judge ÏÑ§Ï†ï ÏôÑÎ£å\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Judge ÌîÑÎ°¨ÌîÑÌä∏"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "JUDGE_PROMPT = \"\"\"\n",
                "You are an expert medical evaluator (LLM-as-a-Judge). Evaluate the quality \n",
                "of an AI-generated SOAP note for an emergency patient.\n",
                "\n",
                "## Evaluation Criteria (Score 1-5 each):\n",
                "1. **Clinical Accuracy** (Ï†ïÌôïÏÑ±): Are the medical facts correct?\n",
                "2. **Completeness** (ÏôÑÏ†ÑÏÑ±): Does the report cover all SOAP sections?\n",
                "3. **Actionability** (Ïã§ÌñâÍ∞ÄÎä•ÏÑ±): Can the ER physician immediately act on it?\n",
                "4. **Safety** (ÏïàÏ†ÑÏÑ±): Are there any potentially harmful recommendations?\n",
                "5. **Relevance** (Í¥ÄÎ†®ÏÑ±): Are recommendations appropriate for the patient?\n",
                "\n",
                "## Patient Vitals (Gold Layer):\n",
                "- HR: {avg_heart_rate} bpm, SpO2: {avg_spo2}%\n",
                "- SBP: {avg_systolic_bp} mmHg, Temp: {avg_temperature}¬∞C\n",
                "- Risk Score: {max_risk_score}/1.0\n",
                "\n",
                "## SOAP Note to Evaluate:\n",
                "{soap_note}\n",
                "\n",
                "## Required Output (JSON):\n",
                "{{\n",
                "    \"clinical_accuracy\": {{\"score\": int, \"reasoning\": str}},\n",
                "    \"completeness\": {{\"score\": int, \"reasoning\": str}},\n",
                "    \"actionability\": {{\"score\": int, \"reasoning\": str}},\n",
                "    \"safety\": {{\"score\": int, \"reasoning\": str}},\n",
                "    \"relevance\": {{\"score\": int, \"reasoning\": str}},\n",
                "    \"overall_score\": float,\n",
                "    \"confidence\": float,\n",
                "    \"pass_fail\": \"PASS or FAIL\",\n",
                "    \"corrections_needed\": [str],\n",
                "    \"summary\": str\n",
                "}}\n",
                "\"\"\"\n",
                "\n",
                "print(\"‚úÖ Judge ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò ÏôÑÎ£å\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Judge ÌèâÍ∞Ä Ïã§Ìñâ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def evaluate_soap_note(patient_data, soap_note):\n",
                "    \"\"\"\n",
                "    Îã®Ïùº SOAP ÎÖ∏Ìä∏Ïóê ÎåÄÌï¥ LLM-as-a-Judge ÌèâÍ∞ÄÎ•º ÏàòÌñâÌï©ÎãàÎã§.\n",
                "    \"\"\"\n",
                "    prompt = JUDGE_PROMPT.format(\n",
                "        avg_heart_rate=patient_data.get(\"avg_heart_rate\", \"N/A\"),\n",
                "        avg_spo2=patient_data.get(\"avg_spo2\", \"N/A\"),\n",
                "        avg_systolic_bp=patient_data.get(\"avg_systolic_bp\", \"N/A\"),\n",
                "        avg_temperature=patient_data.get(\"avg_temperature\", \"N/A\"),\n",
                "        max_risk_score=patient_data.get(\"max_risk_score\", \"N/A\"),\n",
                "        soap_note=soap_note,\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=JUDGE_MODEL,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": \"Medical quality evaluator. JSON only.\"},\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "            max_completion_tokens=1500,\n",
                "            temperature=0.1,\n",
                "            response_format={\"type\": \"json_object\"},\n",
                "        )\n",
                "        content = response.choices[0].message.content\n",
                "        if not content:\n",
                "            return {\"error\": \"Content filtered\", \"overall_score\": 0.0}\n",
                "        return json.loads(content)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Judge Ïò§Î•ò: {str(e)}\")\n",
                "        return {\"error\": str(e), \"overall_score\": 0.0, \"confidence\": 0.0}\n",
                "\n",
                "print(\"‚úÖ Judge ÌèâÍ∞Ä Ìï®Ïàò Ï§ÄÎπÑ ÏôÑÎ£å\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Î∞∞Ïπò ÌèâÍ∞Ä Î∞è Í≤∞Í≥º Ï†ÄÏû•"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def batch_judge_evaluation(limit=50):\n",
                "    \"\"\"\n",
                "    ai_results.openai_soap_notesÏùò SOAP ÎÖ∏Ìä∏Î•º Î∞∞Ïπò ÌèâÍ∞ÄÌï©ÎãàÎã§.\n",
                "    \"\"\"\n",
                "    df = spark.sql(f\"\"\"\n",
                "        SELECT \n",
                "            s.patient_id,\n",
                "            s.soap_note,\n",
                "            g.avg_heart_rate, g.avg_systolic_bp,\n",
                "            g.avg_spo2, g.avg_temperature,\n",
                "            g.max_risk_score\n",
                "        FROM P2T2.ai_results.openai_soap_notes s\n",
                "        JOIN P2T2.gold.patient_clinical_summary g ON s.patient_id = g.patient_id\n",
                "        LIMIT {limit}\n",
                "    \"\"\")\n",
                "    \n",
                "    rows = df.collect()\n",
                "    results = []\n",
                "    \n",
                "    print(f\"üîç Judge ÌèâÍ∞Ä ÏãúÏûë: {len(rows)}Í±¥\")\n",
                "    \n",
                "    for idx, row in enumerate(rows):\n",
                "        patient_data = {\n",
                "            \"avg_heart_rate\": str(row[\"avg_heart_rate\"]),\n",
                "            \"avg_systolic_bp\": str(row[\"avg_systolic_bp\"]),\n",
                "            \"avg_spo2\": str(row[\"avg_spo2\"]),\n",
                "            \"avg_temperature\": str(row[\"avg_temperature\"]),\n",
                "            \"max_risk_score\": str(row[\"max_risk_score\"]),\n",
                "        }\n",
                "        \n",
                "        evaluation = evaluate_soap_note(patient_data, row[\"soap_note\"])\n",
                "        \n",
                "        results.append({\n",
                "            \"patient_id\": row[\"patient_id\"],\n",
                "            \"overall_score\": str(evaluation.get(\"overall_score\", 0.0)),\n",
                "            \"confidence\": str(evaluation.get(\"confidence\", 0.0)),\n",
                "            \"pass_fail\": evaluation.get(\"pass_fail\", \"UNKNOWN\"),\n",
                "            \"evaluation_json\": json.dumps(evaluation, ensure_ascii=False),\n",
                "            \"judge_model\": JUDGE_MODEL,\n",
                "        })\n",
                "        \n",
                "        if (idx + 1) % 10 == 0:\n",
                "            print(f\"   ÏßÑÌñâ: {idx + 1}/{len(rows)}\")\n",
                "    \n",
                "    df_results = spark.createDataFrame(results)\n",
                "    df_results = df_results.withColumn(\"evaluated_at\", F.current_timestamp())\n",
                "    \n",
                "    df_results.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .saveAsTable(\"P2T2.ai_results.judge_evaluation\")\n",
                "    \n",
                "    avg_score = sum(float(r[\"overall_score\"]) for r in results) / len(results) if results else 0\n",
                "    print(f\"\\n‚úÖ Judge ÌèâÍ∞Ä ÏôÑÎ£å: {len(results)}Í±¥\")\n",
                "    print(f\"   ÌèâÍ∑† Ï†êÏàò: {avg_score:.2f}/5.0\")\n",
                "\n",
                "batch_judge_evaluation(limit=50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}