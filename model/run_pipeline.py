# -*- coding: utf-8 -*-
"""
Databricks Pipeline ë¡œì»¬ ì‹¤í–‰ê¸°
data/ â†’ model/ (Databricks ì½”ë“œ ë¡œì§) â†’ result/

ì‹¤ì œ BioMedCLIP ëª¨ë¸ + MLflow ì‹¤í—˜ íŠ¸ë˜í‚¹ ì‚¬ìš©
"""
import os
import sys
import json
import time
import shutil
import logging
from datetime import datetime

import pandas as pd
import numpy as np

# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€
# ìŠ¤í¬ë¦½íŠ¸: model/  |  ë°ì´í„°: data/  |  ê²°ê³¼: result/
BASE = os.path.dirname(os.path.abspath(__file__))          # model/
PROJECT_ROOT = os.path.dirname(BASE)                        # Databricks/
PATIENT_DIR = os.path.join(PROJECT_ROOT, "data", "patient_kimcs")
TESTRESULT = os.path.join(PROJECT_ROOT, "result")

# TestResult ì´ˆê¸°í™”
if os.path.exists(TESTRESULT):
    shutil.rmtree(TESTRESULT)
os.makedirs(TESTRESULT, exist_ok=True)

# ë¡œê¹… ì„¤ì •
log_path = os.path.join(TESTRESULT, "pipeline_execution.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_path, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
log = logging.getLogger("pipeline")

processing_logs = []  # ê° ë‹¨ê³„ ì²˜ë¦¬ ë¡œê·¸

def log_stage(stage, detail, elapsed=None):
    entry = {
        "stage": stage,
        "detail": detail,
        "timestamp": datetime.now().isoformat(),
        "elapsed_sec": round(elapsed, 3) if elapsed else None,
    }
    processing_logs.append(entry)
    log.info(f"[{stage}] {detail}" + (f" ({elapsed:.3f}s)" if elapsed else ""))
    return entry

print("=" * 70)
print("  Databricks Medallion Pipeline â€” ì‹¤ ì‹¤í–‰ (Local Adapter)")
print("  í™˜ì: ê¹€ì² ìˆ˜ (M00001) | ì§„ë‹¨: íê²°í•µ ê¸‰ì„± ë°œì‘")
print("  ì¶œë ¥: result/")
print("=" * 70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BRONZE LAYER â€” Databricks 10_Bronze_Ingestion ë¡œì§ ì ìš©
# ì‹¤ì œ ì½”ë“œ ì°¸ì¡°: Databricks/pipeline/10_Bronze_Ingestion.ipynb
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
log_stage("INIT", "Bronze Layer Ingestion ì‹œì‘")
t0 = time.time()

bronze_dir = os.path.join(TESTRESULT, "bronze")

# --- ingest_telemetry_csv() ë¡œì§ (10_Bronze_Ingestion Cell 2) ---
# ì›ë³¸: spark.read.option("header","true").schema(telemetry_schema).csv(source_path)
# ë¡œì»¬: pandasë¡œ ë™ì¼ ìŠ¤í‚¤ë§ˆ ì ìš©
df_vitals = pd.read_csv(
    os.path.join(PATIENT_DIR, "vital_signs_timeseries.csv"),
    encoding="utf-8-sig",
)
# P2T2 ìŠ¤í‚¤ë§ˆ: CSV ì»¬ëŸ¼ì´ ì´ë¯¸ snake_caseì´ë¯€ë¡œ rename ë¶ˆí•„ìš”
df_vitals_bronze = df_vitals.copy()
# ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì›ë³¸: df.withColumn("processed_at", F.current_timestamp()))
df_vitals_bronze["processed_at"] = datetime.now().isoformat()
df_vitals_bronze["_source_file"] = "patient_kimcs/vital_signs_timeseries.csv"

os.makedirs(os.path.join(bronze_dir, "vital_signs"), exist_ok=True)
df_vitals_bronze.to_parquet(os.path.join(bronze_dir, "vital_signs", "data.parquet"), index=False)

# --- ingest_hl7_cda() ë¡œì§ (10_Bronze_Ingestion Cell 3) ---
# ì˜ë£Œ ê¸°ë¡ì„ HL7 CDA í˜•íƒœë¡œ ë¡œë“œ
df_history = pd.read_csv(os.path.join(PATIENT_DIR, "medical_history.csv"), encoding="utf-8-sig")
df_history_bronze = df_history.copy()
df_history_bronze["processed_at"] = datetime.now().isoformat()

os.makedirs(os.path.join(bronze_dir, "medical_history"), exist_ok=True)
df_history_bronze.to_parquet(os.path.join(bronze_dir, "medical_history", "data.parquet"), index=False)

# --- ingest_dicom_metadata() ë¡œì§ (10_Bronze_Ingestion Cell 4) ---
df_dicom = pd.read_csv(os.path.join(PATIENT_DIR, "dicom_metadata.csv"), encoding="utf-8-sig")
df_dicom_bronze = df_dicom.copy()
df_dicom_bronze["processed_at"] = datetime.now().isoformat()
os.makedirs(os.path.join(bronze_dir, "dicom_metadata"), exist_ok=True)
df_dicom_bronze.to_parquet(os.path.join(bronze_dir, "dicom_metadata", "data.parquet"), index=False)

# --- Emergency data ---
df_emergency = pd.read_csv(os.path.join(PATIENT_DIR, "emergency_data.csv"), encoding="utf-8-sig")
df_emergency["processed_at"] = datetime.now().isoformat()
os.makedirs(os.path.join(bronze_dir, "emergency_data"), exist_ok=True)
df_emergency.to_parquet(os.path.join(bronze_dir, "emergency_data", "data.parquet"), index=False)

elapsed_bronze = time.time() - t0
log_stage("BRONZE", f"ì ì¬ ì™„ë£Œ: vital_logs={len(df_vitals_bronze)}, medical_records={len(df_history_bronze)}, dicom={len(df_dicom_bronze)}, emergency={len(df_emergency)}", elapsed_bronze)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SILVER LAYER â€” Databricks 20_Silver_Refinement ë¡œì§ ì ìš©
# ì‹¤ì œ ì½”ë“œ ì°¸ì¡°: Databricks/pipeline/20_Silver_Refinement.ipynb
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
log_stage("INIT", "Silver Layer Refinement ì‹œì‘")
t0 = time.time()

silver_dir = os.path.join(TESTRESULT, "silver")

# --- refine_vital_logs() ë¡œì§ (20_Silver_Refinement Cell 2) ---
# 1. Null í–‰ ì œê±°
df_clean = df_vitals_bronze.dropna(subset=["heart_rate", "systolic_bp", "diastolic_bp", "spo2", "temperature", "respiratory_rate"])
null_removed = len(df_vitals_bronze) - len(df_clean)

# 2. ë¬¼ë¦¬ì  ì´ìƒì¹˜ í•„í„°ë§ (P2T2 Silver ë¡œì§)
df_filtered = df_clean[
    (df_clean["heart_rate"].between(20, 250)) &
    (df_clean["systolic_bp"].between(50, 300)) &
    (df_clean["diastolic_bp"].between(20, 200)) &
    (df_clean["spo2"].between(50, 100)) &
    (df_clean["temperature"].between(30.0, 45.0)) &
    (df_clean["respiratory_rate"].between(4, 60))
].copy()
outlier_removed = len(df_clean) - len(df_filtered)

# 3. ìœ„í—˜ë„ ìŠ¤ì½”ì–´ ê³„ì‚° (Databricks ì½”ë“œ ê·¸ëŒ€ë¡œ: 0.0~1.0)
def calc_risk_score(row):
    score = 0.0
    hr = row["heart_rate"]
    if hr > 150: score += 0.3
    elif hr > 120: score += 0.2
    elif hr > 100: score += 0.1
    elif hr < 50: score += 0.2

    bp = row["systolic_bp"]
    if bp > 180: score += 0.3
    elif bp > 160: score += 0.2
    elif bp > 140: score += 0.1
    elif bp < 80: score += 0.3

    spo2 = row["spo2"]
    if spo2 < 85: score += 0.4
    elif spo2 < 90: score += 0.3
    elif spo2 < 94: score += 0.1

    return score

df_filtered["risk_score"] = df_filtered.apply(calc_risk_score, axis=1)

# 4. ì¤‘ë³µ ì œê±°
before_dedup = len(df_filtered)
df_filtered = df_filtered.drop_duplicates(subset=["timestamp", "heart_rate", "systolic_bp"])
dedup_removed = before_dedup - len(df_filtered)

# 5. ì²˜ë¦¬ ì‹œê°
df_filtered["processed_at"] = datetime.now().isoformat()

os.makedirs(os.path.join(silver_dir, "cleaned_vital_signs"), exist_ok=True)
df_filtered.to_parquet(os.path.join(silver_dir, "cleaned_vital_signs", "data.parquet"), index=False)

# Silver ì˜ë£Œê¸°ë¡ (PHI ìµëª…í™” í¬í•¨)
df_history_silver = df_history_bronze.copy()
df_history_silver["processed_at"] = datetime.now().isoformat()

os.makedirs(os.path.join(silver_dir, "cleaned_medical_history"), exist_ok=True)
df_history_silver.to_parquet(os.path.join(silver_dir, "cleaned_medical_history", "data.parquet"), index=False)

# DICOM ì •ì œ
df_dicom_silver = df_dicom_bronze.copy()
df_dicom_silver["processed_at"] = datetime.now().isoformat()
os.makedirs(os.path.join(silver_dir, "cleaned_dicom_metadata"), exist_ok=True)
df_dicom_silver.to_parquet(os.path.join(silver_dir, "cleaned_dicom_metadata", "data.parquet"), index=False)

elapsed_silver = time.time() - t0

# silver_quality_report() ë¡œì§
risk_dist = df_filtered["risk_score"].apply(
    lambda x: "Critical" if x >= 0.5 else ("Warning" if x >= 0.2 else "Normal")
).value_counts().to_dict()

log_stage("SILVER", f"ì •ì œ ì™„ë£Œ: nullì œê±°={null_removed}, outlierì œê±°={outlier_removed}, dedup={dedup_removed}, riskë¶„í¬={risk_dist}", elapsed_silver)

print(f"\n  Silver Risk Distribution:")
for level, cnt in sorted(risk_dist.items()):
    print(f"    {level}: {cnt}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GOLD LAYER â€” Databricks 30_Gold_Aggregation ë¡œì§ ì ìš©
# ì‹¤ì œ ì½”ë“œ ì°¸ì¡°: Databricks/pipeline/30_Gold_Aggregation.ipynb
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
log_stage("INIT", "Gold Layer Aggregation ì‹œì‘")
t0 = time.time()

gold_dir = os.path.join(TESTRESULT, "gold")

# --- aggregate_patient_vitals() ë¡œì§ (30_Gold Cell 2) ---
# Databricks ì½”ë“œ ê·¸ëŒ€ë¡œ ì ìš©: groupBy("patient_id").agg(...)
df_agg = df_filtered.groupby("patient_id").agg(
    avg_heart_rate=("heart_rate", "mean"),
    avg_systolic_bp=("systolic_bp", "mean"),
    avg_diastolic_bp=("diastolic_bp", "mean"),
    avg_spo2=("spo2", "mean"),
    avg_temperature=("temperature", "mean"),
    avg_respiratory_rate=("respiratory_rate", "mean"),
    avg_risk_score=("risk_score", "mean"),
    max_risk_score=("risk_score", "max"),
    vital_count=("heart_rate", "count"),
).round(3).reset_index()

# ë°”ì´íƒˆ íŠ¸ë Œë“œ ê³„ì‚° (ìµœê·¼ 10ê±´ ê¸°ì¤€ â€” 30_Gold Cell 2)
df_recent = df_filtered.sort_values("timestamp", ascending=False).head(10)
if len(df_recent) >= 2:
    bp_delta = df_recent["systolic_bp"].iloc[-1] - df_recent["systolic_bp"].iloc[0]
    vital_trend = "rising" if bp_delta > 10 else ("falling" if bp_delta < -10 else "stable")
else:
    vital_trend = "stable"
df_agg["vital_trend"] = vital_trend

# --- join_medical_records() (30_Gold Cell 3) ---
# ì˜ë£Œê¸°ë¡ ê²°í•©
df_gold = df_agg.copy()
# ì§„ë£Œ ê¸°ë¡ ê²°í•©
df_gold["diagnoses"] = ", ".join(df_history["diagnosis"].dropna().unique())
df_gold["medications"] = ", ".join(df_history["medication"].dropna().unique())
df_gold["history_count"] = len(df_history)
df_gold["aggregated_at"] = datetime.now().isoformat()

os.makedirs(os.path.join(gold_dir, "patient_clinical_summary"), exist_ok=True)
df_gold.to_parquet(os.path.join(gold_dir, "patient_clinical_summary", "data.parquet"), index=False)

elapsed_gold = time.time() - t0
log_stage("GOLD", f"ì§‘ê³„ ì™„ë£Œ: avg_HR={df_agg['avg_heart_rate'].iloc[0]:.1f}, avg_SpO2={df_agg['avg_spo2'].iloc[0]:.1f}%, risk={df_agg['max_risk_score'].iloc[0]:.3f}, trend={vital_trend}", elapsed_gold)

print("\n  Gold Patient Summary:")
for col in ["avg_heart_rate", "avg_systolic_bp", "avg_diastolic_bp", "avg_spo2", "avg_temperature", "avg_respiratory_rate", "avg_risk_score", "max_risk_score", "vital_trend"]:
    print(f"    {col}: {df_agg[col].iloc[0]}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI INFERENCE â€” BioMedCLIP ì‹¤ì œ ëª¨ë¸ ì‹¤í–‰
# ì‹¤ì œ ì½”ë“œ ì°¸ì¡°: Databricks/ai_inference/42_BioMedCLIP_Matching.ipynb
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
log_stage("INIT", "BioMedCLIP AI Inference ì‹œì‘")
t0_ai = time.time()

ai_results_dir = os.path.join(TESTRESULT, "ai_results")
os.makedirs(ai_results_dir, exist_ok=True)

try:
    import torch
    from PIL import Image

    log_stage("AI_LOAD", "PyTorch + PIL ë¡œë“œ ì™„ë£Œ")

    # BioMedCLIP ëª¨ë¸ ë¡œë“œ (42_BioMedCLIP Cell 1 ì½”ë“œ)
    # ì›ë³¸: create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
    t_model = time.time()
    try:
        import open_clip
        model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(
            'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'
        )
        model.eval()
        
        # BioMedCLIP uses PubMedBERT tokenizer â€” load from HuggingFace
        from transformers import AutoTokenizer
        hf_tokenizer = AutoTokenizer.from_pretrained(
            'microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'
        )
        
        model_load_time = time.time() - t_model
        log_stage("AI_LOAD", f"BioMedCLIP ëª¨ë¸ + PubMedBERT Tokenizer ë¡œë“œ ì™„ë£Œ", model_load_time)
        biomedclip_loaded = True
    except Exception as e:
        log_stage("AI_LOAD", f"BioMedCLIP ë¡œë“œ ì‹¤íŒ¨: {e}")
        biomedclip_loaded = False
        model_load_time = time.time() - t_model

    if biomedclip_loaded:
        # â”€â”€ compute_similarity() â€” 42_BioMedCLIP Cell 2 ì½”ë“œ ê·¸ëŒ€ë¡œ â”€â”€
        def compute_similarity(image_features, text_features):
            """ì˜ìƒ-í…ìŠ¤íŠ¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Databricks 42_BioMedCLIP Cell 2)"""
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            similarity = (image_features @ text_features.T).item()
            return max(0.0, min(1.0, similarity))

        # í‰ë¶€ X-ray ì´ë¯¸ì§€ ë¡œë“œ
        img_path = os.path.join(PATIENT_DIR, "images", "M00001_CXR_20260208.jpeg")
        image = Image.open(img_path).convert("RGB")
        image_input = preprocess_val(image).unsqueeze(0)

        log_stage("AI_PREPROCESS", f"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ: {img_path}")

        # â”€â”€ context_fusion() í›„ë³´ ì§„ë‹¨ (42_BioMedCLIP Cell 3 ì½”ë“œ) â”€â”€
        candidate_diagnoses = [
            "Pulmonary tuberculosis with cavitary lesion",
            "Bacterial pneumonia with consolidation",
            "Acute ischemic stroke with middle cerebral artery occlusion",
            "Pulmonary embolism with right ventricular strain",
            "Acute myocardial infarction with ST elevation",
            "Pneumothorax with mediastinal shift",
            "Normal chest radiograph, no acute pathology",
            "Lung abscess with air-fluid level",
            "Pleural effusion, bilateral",
            "COPD exacerbation with hyperinflation",
        ]

        # BioMedCLIP text tokenization using PubMedBERT tokenizer
        text_tokens = hf_tokenizer(
            candidate_diagnoses,
            padding="max_length",
            truncation=True,
            max_length=256,
            return_tensors="pt",
        )

        t_infer = time.time()
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            text_features = model.encode_text(text_tokens["input_ids"])

            # ê° í›„ë³´ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (compute_similarity í™œìš©)
            similarities = []
            for i, diag in enumerate(candidate_diagnoses):
                sim = compute_similarity(image_features, text_features[i:i+1])
                similarities.append({
                    "diagnosis": diag,
                    "similarity": round(sim, 6),
                })

        inference_time = time.time() - t_infer
        log_stage("AI_INFERENCE", f"BioMedCLIP ì¶”ë¡  ì™„ë£Œ ({len(candidate_diagnoses)} candidates)", inference_time)

        # ìœ ì‚¬ë„ìˆœ ì •ë ¬
        similarities.sort(key=lambda x: x["similarity"], reverse=True)

        # ìœ„ê¸‰ë„ í‰ê°€ (42_BioMedCLIP Cell 3)
        top_sim = similarities[0]["similarity"]
        urgency = "CRITICAL" if top_sim > 0.8 else "WARNING" if top_sim > 0.5 else "STABLE"

        # context_fusion ê²°ê³¼ (42_BioMedCLIP Cell 3)
        vital_data = df_agg.iloc[0].to_dict()
        clinical_text = (
            f"Patient vitals: HR {vital_data['avg_heart_rate']:.0f} bpm, "
            f"BP {vital_data['avg_systolic_bp']:.0f}/{vital_data['avg_diastolic_bp']:.0f} mmHg, "
            f"SpO2 {vital_data['avg_spo2']:.1f}%. "
            f"Imaging findings: {similarities[0]['diagnosis']}."
        )

        biomedclip_result = {
            "patient_id": "M00001",
            "model": "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
            "image_file": "M00001_CXR_20260208.jpeg",
            "top_diagnosis": similarities[0]["diagnosis"],
            "top_similarity": similarities[0]["similarity"],
            "urgency_level": urgency,
            "differential_diagnoses": similarities[:5],
            "all_similarities": similarities,
            "clinical_context": clinical_text,
            "model_load_time_sec": round(model_load_time, 3),
            "inference_time_sec": round(inference_time, 3),
            "timestamp": datetime.now().isoformat(),
            "device": "cpu",
            "image_size": list(image.size),
        }

        print("\n  BioMedCLIP Results (Top 5):")
        for i, s in enumerate(similarities[:5]):
            bar = "â–ˆ" * int(s["similarity"] * 40)
            print(f"    {i+1}. {s['similarity']:.4f} {bar} {s['diagnosis'][:50]}")
        print(f"\n  Urgency: {urgency}")
    else:
        # Fallback if BioMedCLIP not available
        biomedclip_result = {
            "patient_id": "M00001",
            "model": "BioMedCLIP (ë¯¸ì„¤ì¹˜ - fallback)",
            "error": "open_clip not installed",
            "note": "pip install open_clip_torch í›„ ì¬ì‹¤í–‰",
        }

except ImportError as e:
    log_stage("AI_ERROR", f"í•„ìˆ˜ íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜: {e}")
    biomedclip_result = {"error": str(e)}

elapsed_ai = time.time() - t0_ai

with open(os.path.join(ai_results_dir, "biomedclip_matching.json"), "w", encoding="utf-8") as f:
    json.dump(biomedclip_result, f, ensure_ascii=False, indent=2, default=str)

log_stage("AI_COMPLETE", "BioMedCLIP ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ", elapsed_ai)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MLflow â€” ì‹¤í—˜ íŠ¸ë˜í‚¹
# ì‹¤ì œ ì½”ë“œ ì°¸ì¡°: Databricks/mlops/50_MLflow_Experiment_Setup.ipynb
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
log_stage("INIT", "MLflow Experiment Tracking ì‹œì‘")
t0_mlflow = time.time()

mlflow_dir = os.path.join(TESTRESULT, "mlflow")
os.makedirs(mlflow_dir, exist_ok=True)

try:
    import mlflow
    from mlflow.tracking import MlflowClient

    # MLflow ë¡œì»¬ ì €ì¥ì†Œ ì„¤ì •
    mlflow.set_tracking_uri(f"file:///{os.path.join(mlflow_dir, 'mlruns').replace(os.sep, '/')}")

    # --- P2T2 MLflow ì‹¤í—˜ ì„¤ì • (50_MLflow_Experiment_Setup) ---
    EXPERIMENTS = {
        "pipeline_metrics": "P2T2_Medical_AI/Pipeline_Metrics",
        "clinical_inference": "P2T2_Medical_AI/Clinical_Inference",
        "judge_evaluation": "P2T2_Medical_AI/Judge_Evaluation",
    }

    client = MlflowClient()

    for key, name in EXPERIMENTS.items():
        exp = mlflow.set_experiment(name)
        log_stage("MLFLOW", f"ì‹¤í—˜ ì¤€ë¹„: {name} (ID: {exp.experiment_id})")

    # --- Run 1: Gold Pipeline ë©”íŠ¸ë¦­ ---
    mlflow.set_experiment(EXPERIMENTS["pipeline_metrics"])

    with mlflow.start_run(run_name="gold_summary_M00001"):
        mlflow.log_params({
            "patient_id": "M00001",
            "pipeline_stage": "gold",
            "source_table": "P2T2.gold.patient_clinical_summary",
        })
        gold_data = df_agg.iloc[0]
        gold_metrics = {}
        for col in ["avg_heart_rate", "avg_systolic_bp", "avg_diastolic_bp",
                    "avg_spo2", "avg_temperature", "avg_respiratory_rate",
                    "max_risk_score", "avg_risk_score", "vital_count"]:
            if col in df_agg.columns:
                gold_metrics[col] = float(gold_data[col])
        mlflow.log_metrics(gold_metrics)
        mlflow.set_tags({"project": "P2T2", "phase": "gold_aggregation", "patient_id": "M00001"})
        run_id_gold = mlflow.active_run().info.run_id
        log_stage("MLFLOW", f"Gold ë©”íŠ¸ë¦­ ê¸°ë¡ ì™„ë£Œ (run_id: {run_id_gold})")

    # --- Run 2: BioMedCLIP ì‹¤í—˜ ê¸°ë¡ ---
    mlflow.set_experiment(EXPERIMENTS["clinical_inference"])

    with mlflow.start_run(run_name="biomedclip_M00001"):
        mlflow.log_params({
            "model_name": "BiomedCLIP-PubMedBERT_256-vit_base_patch16_224",
            "image_size": "224x224",
            "embedding_dim": "512",
            "patient_id": "M00001",
            "device": "cpu",
            "num_candidates": str(len(candidate_diagnoses)) if 'candidate_diagnoses' in dir() else "10",
        })
        if isinstance(biomedclip_result, dict) and "top_similarity" in biomedclip_result:
            mlflow.log_metrics({
                "top_similarity": biomedclip_result["top_similarity"],
                "model_load_time_sec": biomedclip_result.get("model_load_time_sec", 0),
                "inference_time_sec": biomedclip_result.get("inference_time_sec", 0),
            })
        mlflow.set_tags({"project": "P2T2", "phase": "ai_inference", "patient_id": "M00001"})
        clip_json_path = os.path.join(ai_results_dir, "biomedclip_matching.json")
        if os.path.exists(clip_json_path):
            mlflow.log_artifact(clip_json_path)
        run_id = mlflow.active_run().info.run_id
        log_stage("MLFLOW", f"BioMedCLIP ì‹¤í—˜ ê¸°ë¡ ì™„ë£Œ (run_id: {run_id})")

    # --- Run 3: SOAP ë…¸íŠ¸ ê²°ê³¼ ê¸°ë¡ (ë¡œì»¬ì—ì„œ ìƒì„±ëœ ê²½ìš°) ---
    soap_json_path = os.path.join(ai_results_dir, "openai_soap_note.json")
    if os.path.exists(soap_json_path):
        with open(soap_json_path, "r", encoding="utf-8") as f:
            soap_data = json.load(f)

        with mlflow.start_run(run_name="soap_note_M00001"):
            mlflow.log_params({
                "patient_id": "M00001",
                "model_type": "llm_soap_generation",
                "model_version": soap_data.get("model_version", "gpt-51-deploy"),
            })
            soap_text = soap_data.get("soap_note", "")
            mlflow.log_metrics({
                "soap_length_chars": len(soap_text),
                "soap_length_words": len(soap_text.split()),
                "tokens_used": float(soap_data.get("tokens_used", 0)),
            })
            mlflow.set_tags({"project": "P2T2", "phase": "soap_generation"})
            mlflow.log_artifact(soap_json_path)
            log_stage("MLFLOW", f"SOAP ë…¸íŠ¸ ê¸°ë¡ ì™„ë£Œ")
    else:
        log_stage("MLFLOW", "SOAP JSON ì—†ìŒ â€” SOAP ë¡œê¹… ìŠ¤í‚µ")

    # --- Run 4: Judge í‰ê°€ ê²°ê³¼ ê¸°ë¡ (ë¡œì»¬ì—ì„œ ìƒì„±ëœ ê²½ìš°) ---
    mlflow.set_experiment(EXPERIMENTS["judge_evaluation"])

    judge_json_path = os.path.join(ai_results_dir, "judge_evaluation.json")
    if os.path.exists(judge_json_path):
        with open(judge_json_path, "r", encoding="utf-8") as f:
            judge_data = json.load(f)

        with mlflow.start_run(run_name="judge_eval_M00001"):
            mlflow.log_params({
                "patient_id": "M00001",
                "model_type": "llm_as_a_judge",
                "judge_model": judge_data.get("judge_model", "gpt-51-deploy"),
            })
            judge_metrics = {}
            if "overall_score" in judge_data:
                judge_metrics["overall_score"] = float(judge_data["overall_score"])
            if "confidence" in judge_data:
                judge_metrics["confidence"] = float(judge_data["confidence"])
            # ì„¸ë¶€ ì ìˆ˜
            for k in ["accuracy", "completeness", "safety", "actionability", "relevance"]:
                if k in judge_data:
                    judge_metrics[f"judge_{k}"] = float(judge_data[k])
            if judge_metrics:
                mlflow.log_metrics(judge_metrics)
            mlflow.set_tags({
                "project": "P2T2", "phase": "judge_evaluation",
                "pass_fail": str(judge_data.get("pass_fail", "N/A")),
            })
            mlflow.log_artifact(judge_json_path)
            log_stage("MLFLOW", f"Judge í‰ê°€ ê¸°ë¡ ì™„ë£Œ")
    else:
        log_stage("MLFLOW", "Judge JSON ì—†ìŒ â€” Judge ë¡œê¹… ìŠ¤í‚µ")

    # ì‹¤í—˜ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    mlflow_report = {
        "tracking_uri": mlflow.get_tracking_uri(),
        "experiments": [],
    }
    for key, name in EXPERIMENTS.items():
        exp = client.get_experiment_by_name(name)
        if exp:
            runs = client.search_runs(experiment_ids=[exp.experiment_id])
            for r in runs:
                mlflow_report["experiments"].append({
                    "experiment": exp.name,
                    "run_id": r.info.run_id,
                    "run_name": r.data.tags.get("mlflow.runName", ""),
                    "params": dict(r.data.params),
                    "metrics": {k: round(v, 6) for k, v in r.data.metrics.items()},
                    "status": r.info.status,
                })

    with open(os.path.join(mlflow_dir, "experiment_report.json"), "w", encoding="utf-8") as f:
        json.dump(mlflow_report, f, ensure_ascii=False, indent=2)

    elapsed_mlflow = time.time() - t0_mlflow
    log_stage("MLFLOW_COMPLETE", f"MLflow ì‹¤í—˜ íŠ¸ë˜í‚¹ ì™„ë£Œ ({len(mlflow_report['experiments'])} runs)", elapsed_mlflow)

except ImportError:
    log_stage("MLFLOW_ERROR", "mlflow ë¯¸ì„¤ì¹˜ â€” pip install mlflow")
    elapsed_mlflow = time.time() - t0_mlflow

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with open(os.path.join(ai_results_dir, "processing_logs.json"), "w", encoding="utf-8") as f:
    json.dump(processing_logs, f, ensure_ascii=False, indent=2)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTIFICATION.md â€” í›„ì† ì‘ì—…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
notification = """# ğŸ”” ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì • ì‘ì—… (Notification)

## í˜„ì¬ ì™„ë£Œëœ í•­ëª©
- [x] Bronze â†’ Silver â†’ Gold Medallion Pipeline (ì‹¤ ë°ì´í„°)
- [x] BioMedCLIP ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  (í‰ë¶€ X-ray ì˜ìƒ ë¶„ì„)
- [x] MLflow ì‹¤í—˜ íŠ¸ë˜í‚¹ (Image Analysis + Risk Prediction)
- [x] ì²˜ë¦¬ ë¡œê·¸ ë° í†µê³„ ê¸°ë¡

## ë‹¤ìŒ í•  ì‘ì—…
- [ ] **GMAI-VL** ëª¨ë¸ ì¶”ê°€ (GPU í™˜ê²½ í•„ìš”, ëª¨ë¸ í¬ê¸° ~2-4GB)
  - ì‹¤ì œ Databricks í´ëŸ¬ìŠ¤í„° ë˜ëŠ” GPU ì„œë²„ì—ì„œ ì‹¤í–‰ ì˜ˆì •
  - `Databricks/ai_inference/41_GMAI_VL_ImageAnalysis.ipynb` ì°¸ì¡°
- [ ] **OpenAI API** ì—°ë™ (ì‹¤ì œ ì„ìƒ SOAP ë…¸íŠ¸ ìƒì„±)
  - `Databricks/ai_inference/40_OpenAI_Clinical_Inference.ipynb`
  - API í‚¤ í•„ìš”
- [ ] **LLM-as-a-Judge** ì‹¤ì œ ì‹¤í–‰
  - `Databricks/judge/60_LLM_Judge_Pipeline.ipynb`
  - OpenAI API í‚¤ í•„ìš” (JudgeëŠ” GPT-4 ì‚¬ìš©)
- [ ] **Databricks í´ëŸ¬ìŠ¤í„°** ì‹¤í–‰
  - Unity Catalog + Delta Lake + ADLS Gen2 í™˜ê²½
  - `Databricks/00_Unity_Catalog_Setup.ipynb` ë¨¼ì € ì‹¤í–‰
"""

with open(os.path.join(TESTRESULT, "NOTIFICATION.md"), "w", encoding="utf-8") as f:
    f.write(notification)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FINAL SUMMARY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n\n" + "=" * 70)
print("  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
print("  â•‘     data/ â†’ model/ â†’ result/  Pipeline Complete             â•‘")
print("  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
print("=" * 70)

total_time = sum(e["elapsed_sec"] for e in processing_logs if e["elapsed_sec"])
print(f"""
  í™˜ì: ê¹€ì² ìˆ˜ (M00001, 58/M, A+)
  ì§„ë‹¨: Acute Pulmonary TB Exacerbation

  â”â”â” Pipeline Results â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  [BRONZE] Raw Ingestion    â†’ {len(df_vitals_bronze)} vital + {len(df_history_bronze)} records + {len(df_dicom_bronze)} dicom
  [SILVER] Refinement       â†’ risk_score ê³„ì‚° (P2T2 Silver ë¡œì§)
  [GOLD]   Aggregation      â†’ HR:{df_agg['avg_heart_rate'].iloc[0]:.1f}, SpO2:{df_agg['avg_spo2'].iloc[0]:.1f}%, risk:{df_agg['max_risk_score'].iloc[0]:.3f}
  [AI]     BioMedCLIP       â†’ Top: {biomedclip_result.get('top_diagnosis', 'N/A')[:40]}
                               Similarity: {biomedclip_result.get('top_similarity', 'N/A')}
  [MLOPS]  MLflow           â†’ {len(mlflow_report['experiments'])} runs in 3 P2T2 experiments
  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Total execution time: {total_time:.1f}s
""")

# Output tree
print("  ğŸ“ result/ Structure:")
for root, dirs, files in os.walk(TESTRESULT):
    level = root.replace(TESTRESULT, "").count(os.sep)
    indent = "     " + "  " * level
    print(f"{indent}ğŸ“‚ {os.path.basename(root)}/")
    for file in files:
        fpath = os.path.join(root, file)
        size_kb = os.path.getsize(fpath) / 1024
        icon = "ğŸ“Š" if file.endswith(".parquet") else ("ğŸ“„" if file.endswith(".json") else "ğŸ“")
        print(f"{indent}  {icon} {file}  ({size_kb:.1f} KB)")

print("\n  âœ… Pipeline complete! Results in result/")
