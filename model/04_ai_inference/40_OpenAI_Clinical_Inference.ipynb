{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Azure OpenAI ì„ìƒ ì¶”ë¡  + SOAP ë…¸íŠ¸ ìƒì„±\n",
                "\n",
                "> **Purpose:** Gold Layerì˜ í™˜ì ë°ì´í„°ë¥¼ Azure OpenAI APIì— ì „ë‹¬í•˜ì—¬\n",
                "> SOAP í˜•ì‹ì˜ ì§„ë£Œ ê°€ì´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "> - í™˜ì ë°”ì´íƒˆ + ì§„ë£Œ ê¸°ë¡ í†µí•© í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
                "> - Azure OpenAI API í˜¸ì¶œ (GPT-5.1 / GPT-4o)\n",
                "> - ê²°ê³¼ë¥¼ `P2T2.ai_results.openai_soap_notes` í…Œì´ë¸”ì— ì €ì¥\n",
                ">\n",
                "> **ì¹´íƒˆë¡œê·¸:** `P2T2`\n",
                "> **ìŠ¤í‚¤ë§ˆ:** `ai_results`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. ì¹´íƒˆë¡œê·¸ ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"USE CATALOG P2T2\")\n",
                "print(\"âœ… Catalog: P2T2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import openai\n",
                "import json\n",
                "import time\n",
                "from pyspark.sql import functions as F\n",
                "from pyspark.sql.types import *\n",
                "\n",
                "# Azure Key Vaultì—ì„œ API í‚¤ ë¡œë“œ\n",
                "OPENAI_API_KEY = dbutils.secrets.get(scope=\"key-vault-scope\", key=\"openai-api-key\")\n",
                "OPENAI_ENDPOINT = dbutils.secrets.get(scope=\"key-vault-scope\", key=\"openai-endpoint\")\n",
                "\n",
                "# Azure OpenAI í´ë¼ì´ì–¸íŠ¸\n",
                "client = openai.AzureOpenAI(\n",
                "    azure_endpoint=OPENAI_ENDPOINT,\n",
                "    api_key=OPENAI_API_KEY,\n",
                "    api_version=\"2025-01-01-preview\"\n",
                ")\n",
                "\n",
                "MODEL = \"gpt-51-deploy\"  # Azure OpenAI ë°°í¬ëª…\n",
                "MAX_TOKENS = 1500\n",
                "TEMPERATURE = 0.3  # ì˜ë£Œ: ë‚®ì€ temperatureë¡œ ì¼ê´€ì„± í™•ë³´\n",
                "\n",
                "print(f\"ğŸ”§ Azure OpenAI: Model={MODEL}, MaxTokens={MAX_TOKENS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. SOAP í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "SOAP_PROMPT = \"\"\"\n",
                "You are an experienced emergency medicine AI assistant. Generate a SOAP note\n",
                "for the following patient data.\n",
                "\n",
                "## Patient Information\n",
                "- Patient ID: {patient_id}\n",
                "- Diagnoses: {diagnoses}\n",
                "- Medications: {medications}\n",
                "\n",
                "## Vital Signs Summary (Silver â†’ Gold aggregated)\n",
                "- Heart Rate: {avg_heart_rate} bpm\n",
                "- Systolic BP: {avg_systolic_bp} mmHg\n",
                "- Diastolic BP: {avg_diastolic_bp} mmHg\n",
                "- SpO2: {avg_spo2}%\n",
                "- Temperature: {avg_temperature}Â°C\n",
                "- Respiratory Rate: {avg_respiratory_rate} /min\n",
                "- Risk Score: {max_risk_score}/1.0\n",
                "\n",
                "## Output Format\n",
                "### S (Subjective)\n",
                "### O (Objective)\n",
                "### A (Assessment)\n",
                "### P (Plan)\n",
                "\"\"\"\n",
                "\n",
                "print(\"âœ… SOAP í”„ë¡¬í”„íŠ¸ ì •ì˜ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ì¶”ë¡  ì‹¤í–‰"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def run_soap_inference(patient_row):\n",
                "    \"\"\"\n",
                "    ë‹¨ì¼ í™˜ìì— ëŒ€í•´ SOAP ë…¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
                "    \n",
                "    Args:\n",
                "        patient_row: Gold Layer patient_clinical_summary Row\n",
                "    \"\"\"\n",
                "    prompt = SOAP_PROMPT.format(\n",
                "        patient_id=patient_row[\"patient_id\"],\n",
                "        diagnoses=patient_row.get(\"diagnoses\", \"N/A\"),\n",
                "        medications=patient_row.get(\"medications\", \"N/A\"),\n",
                "        avg_heart_rate=patient_row[\"avg_heart_rate\"],\n",
                "        avg_systolic_bp=patient_row[\"avg_systolic_bp\"],\n",
                "        avg_diastolic_bp=patient_row.get(\"avg_diastolic_bp\", \"N/A\"),\n",
                "        avg_spo2=patient_row[\"avg_spo2\"],\n",
                "        avg_temperature=patient_row.get(\"avg_temperature\", \"N/A\"),\n",
                "        avg_respiratory_rate=patient_row.get(\"avg_respiratory_rate\", \"N/A\"),\n",
                "        max_risk_score=patient_row[\"max_risk_score\"],\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=MODEL,\n",
                "            messages=[\n",
                "                {\"role\": \"system\", \"content\": \"You are a clinical decision support AI for emergency medicine.\"},\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "            max_completion_tokens=MAX_TOKENS,\n",
                "            temperature=TEMPERATURE,\n",
                "        )\n",
                "        soap_note = response.choices[0].message.content or \"[Content filtered]\"\n",
                "        tokens = response.usage.total_tokens if response.usage else 0\n",
                "        return {\n",
                "            \"patient_id\": patient_row[\"patient_id\"],\n",
                "            \"soap_note\": soap_note,\n",
                "            \"model_version\": MODEL,\n",
                "            \"tokens_used\": str(tokens),\n",
                "        }\n",
                "    except Exception as e:\n",
                "        print(f\"âš ï¸ API ì˜¤ë¥˜ ({patient_row['patient_id']}): {str(e)}\")\n",
                "        return {\n",
                "            \"patient_id\": patient_row[\"patient_id\"],\n",
                "            \"soap_note\": f\"Error: {str(e)}\",\n",
                "            \"model_version\": MODEL,\n",
                "            \"tokens_used\": \"0\",\n",
                "        }\n",
                "\n",
                "print(\"âœ… SOAP ì¶”ë¡  í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ë°°ì¹˜ ì¶”ë¡  ë° ì €ì¥"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def batch_soap_inference(limit=100, delay=0.5):\n",
                "    \"\"\"\n",
                "    Gold Layer í™˜ì ë°ì´í„°ì— ëŒ€í•´ ë°°ì¹˜ SOAP ì¶”ë¡  í›„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
                "    \"\"\"\n",
                "    df_patients = spark.sql(f\"\"\"\n",
                "        SELECT * FROM P2T2.gold.patient_clinical_summary\n",
                "        ORDER BY max_risk_score DESC\n",
                "        LIMIT {limit}\n",
                "    \"\"\")\n",
                "    \n",
                "    patients = df_patients.collect()\n",
                "    results = []\n",
                "    \n",
                "    print(f\"ğŸš€ ë°°ì¹˜ SOAP ì¶”ë¡  ì‹œì‘: {len(patients)}ëª…\")\n",
                "    \n",
                "    for idx, patient in enumerate(patients):\n",
                "        result = run_soap_inference(patient)\n",
                "        results.append(result)\n",
                "        if (idx + 1) % 10 == 0:\n",
                "            print(f\"   ì§„í–‰: {idx + 1}/{len(patients)}\")\n",
                "        time.sleep(delay)\n",
                "    \n",
                "    df_results = spark.createDataFrame(results)\n",
                "    df_results = df_results.withColumn(\"created_at\", F.current_timestamp())\n",
                "    \n",
                "    df_results.write \\\n",
                "        .format(\"delta\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"overwriteSchema\", \"true\") \\\n",
                "        .saveAsTable(\"P2T2.ai_results.openai_soap_notes\")\n",
                "    \n",
                "    print(f\"\\nâœ… SOAP ì¶”ë¡  ì™„ë£Œ: {len(results)}ê±´ â†’ P2T2.ai_results.openai_soap_notes\")\n",
                "\n",
                "batch_soap_inference(limit=100, delay=1.0)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}